# Observational Memory : une m√©moire bio-inspir√©e pour agents IA

## M√©tadonn√©es
- **Source** : [Mastra ‚Äî Observational Memory](https://x.com/i/article/2021152822264201218)
- **Type** : article
- **Auteur** : Tyler Barnes (Mastra)
- **Date publication** : 2026-02-09
- **Date ajout** : 2026-02-11
- **Domaine** : pro
- **Tags** : #agents #m√©moire #contexte-long #benchmark #LongMemEval #prompt-caching #open-source
- **Confiance** : ‚≠ê‚≠ê‚≠ê‚≠ê (benchmarks publics reproductibles, SoTA sur LongMemEval, code open-source, repris par VentureBeat)

## TL;DR
> Mastra lance Observational Memory, un syst√®me de m√©moire textuel pour agents IA qui atteint 94,87% sur LongMemEval (nouveau SoTA) sans n√©cessiter de base vectorielle ni de graph DB. Deux agents de fond (Observer et Reflector) compriment les conversations en observations structur√©es, r√©duisant les co√ªts d'un facteur 3-40x gr√¢ce √† la compatibilit√© avec le prompt caching.

## Points cl√©s
- **Textuel pur** : pas de vector DB ni de graph DB ‚Äî la m√©moire est stock√©e sous forme de texte structur√©, ce qui la rend d√©bugable et portable
- **SoTA LongMemEval** : 94,87% avec gpt-5-mini, battant l'oracle gpt-4o de 2 points et le pr√©c√©dent SoTA (Supermemory, 81,60%) de 13 points
- **Compression 3-40x** : ratio de 3-6x pour du texte conversationnel, jusqu'√† 5-40x pour des agents √† usage intensif d'outils
- **Prompt-cacheable** : architecture √† deux blocs (observations + messages bruts) compatible avec le caching Anthropic/OpenAI

## Concepts & M√©thodologies

### Architecture √† deux blocs
Le contexte est divis√© en deux parties :
1. **Bloc Observations** : r√©sum√©s structur√©s et compress√©s des interactions pass√©es
2. **Bloc Messages bruts** : messages r√©cents non trait√©s

### M√©canisme Observer/Reflector
- **Observer** : quand les messages bruts d√©passent un seuil (d√©faut : 30K tokens), l'Observer les comprime en observations concises ajout√©es au premier bloc
- **Reflector** : quand les observations d√©passent leur seuil (d√©faut : 40K tokens), le Reflector fait le m√©nage ‚Äî fusionne les observations li√©es, supprime l'obsol√®te

### Format des observations
Les observations utilisent un format texte structur√© avec :
- **Trois dates** : date d'observation, date r√©f√©renc√©e, date relative ‚Äî pour le raisonnement temporel
- **Priorit√© par emoji** : üî¥ (important), üü° (peut-√™tre important), üü¢ (info)
- **Hi√©rarchie** : sous-observations indent√©es sous l'observation parente

### Optimisation du cache
Les messages s'accumulent jusqu'au seuil ‚Üí cache hits complets √† chaque tour. Apr√®s l'Observer, le pr√©fixe stable permet des cache hits partiels. Seul le Reflector invalide le cache entier.

‚Üí Voir aussi : [Recursive Language Models](2026-02-11-recursive-language-models.md) ‚Äî approche compl√©mentaire pour le contexte long, o√π le contexte est externalis√© dans un REPL Python plut√¥t que compress√© en observations

## Pense-b√™tes
- √âvaluer Mastra Memory pour des agents conversationnels √† long terme ‚Äî l'activation est une seule ligne de config (`observationalMemory: true`)
- Le format textuel rend le debugging beaucoup plus simple que les embeddings vectoriels ‚Äî on peut lire et auditer la m√©moire directement
- Attention au mode synchrone actuel : l'Observer bloque la conversation quand le seuil est atteint (mode async en cours de d√©ploiement)
- Les seuils Observer (30K) et Reflector (40K) sont configurables ‚Äî √† ajuster selon le use case
- Compl√©mentaire aux approches RAG classiques : OM pour la m√©moire conversationnelle, RAG pour la knowledge base statique

## Citation notable
> "We process millions of pieces of sensory data and distill them down to one or two observations." ‚Äî Tyler Barnes, sur l'inspiration cognitive de l'Observational Memory

## Contenu d√©taill√©

Observational Memory (OM) est un syst√®me de m√©moire pour agents IA d√©velopp√© par Mastra, inspir√© du fonctionnement de la m√©moire humaine. Au lieu de stocker l'int√©gralit√© des √©changes ou de les vectoriser dans une base externe, OM utilise deux agents de fond ‚Äî l'Observer et le Reflector ‚Äî pour maintenir une fen√™tre de contexte stable et comprim√©e.

**Architecture** : le contexte de l'agent est divis√© en deux blocs. Le bloc d'observations contient des r√©sum√©s structur√©s des interactions pass√©es sous forme de texte. Le bloc de messages bruts contient la conversation r√©cente. Cette s√©paration permet d'exploiter le prompt caching des providers (Anthropic, OpenAI) : tant que les observations ne changent pas, le pr√©fixe du prompt est identique et b√©n√©ficie du cache.

**M√©canisme de compression** : quand les messages bruts atteignent 30K tokens (configurable), l'Observer entre en action et cr√©e des observations ‚Äî des notes concises dat√©es et prioris√©es. Chaque observation porte trois dates (cr√©ation, r√©f√©rence, relative) pour le raisonnement temporel, et un niveau de priorit√© (üî¥/üü°/üü¢). Quand les observations elles-m√™mes d√©passent 40K tokens, le Reflector les condense en fusionnant les items li√©s et en √©liminant l'obsol√®te.

**Benchmarks** : sur LongMemEval, le benchmark de r√©f√©rence pour la m√©moire conversationnelle longue, OM avec gpt-5-mini atteint 94,87% ‚Äî le score le plus √©lev√© jamais enregistr√©, surpassant l'oracle gpt-4o (qui ne re√ßoit que les conversations contenant la r√©ponse) de 2 points. Avec gpt-4o comme mod√®le, OM atteint 84,23%, battant Supermemory (81,60%) et le full-context brut (60,20%).

**Avantages pratiques** : OM offre une compression de 3-6x sur du texte conversationnel et de 5-40x sur des agents √† usage intensif d'outils. L'absence de d√©pendance externe (pas de vector DB, pas de graph DB) simplifie le d√©ploiement et le debugging. Le format texte permet d'inspecter directement la m√©moire de l'agent.

**Limitation actuelle** : le traitement par l'Observer est synchrone ‚Äî il bloque la conversation quand le seuil est atteint. Un mode asynchrone avec buffering en arri√®re-plan est en cours de d√©ploiement. L'impl√©mentation est enti√®rement open-source dans le framework Mastra.
